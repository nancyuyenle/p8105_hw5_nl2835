p8105_hw5_nl2835
================
Nancy Le
2023-11-12

``` r
homicides <-
  read.csv("./local/homicide-data.csv") |> 
  janitor::clean_names() |> 
  separate(reported_date, into = c("year", "month", "day"), sep = c(4,6))  
```

## Problem 1

The raw data describes victim name, race, age, sex, location of
homicide, and whether an arrest was made.

``` r
homicides = homicides |> 
  mutate(city_state = paste(city, state, sep = ", ")) |> 
  group_by(city_state) 
```

``` r
homicides |> 
  summarize(count = n())
## # A tibble: 51 × 2
##    city_state      count
##    <chr>           <int>
##  1 Albuquerque, NM   378
##  2 Atlanta, GA       973
##  3 Baltimore, MD    2827
##  4 Baton Rouge, LA   424
##  5 Birmingham, AL    800
##  6 Boston, MA        614
##  7 Buffalo, NY       521
##  8 Charlotte, NC     687
##  9 Chicago, IL      5535
## 10 Cincinnati, OH    694
## # ℹ 41 more rows
```

There are 52179 homicides from 2010 to 2016.

``` r
homicides |> 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") |> 
  summarize(count = n())
## # A tibble: 50 × 2
##    city_state      count
##    <chr>           <int>
##  1 Albuquerque, NM   146
##  2 Atlanta, GA       373
##  3 Baltimore, MD    1825
##  4 Baton Rouge, LA   196
##  5 Birmingham, AL    347
##  6 Boston, MA        310
##  7 Buffalo, NY       319
##  8 Charlotte, NC     206
##  9 Chicago, IL      4073
## 10 Cincinnati, OH    309
## # ℹ 40 more rows
```

There are 26505 unsolved homicides.

``` r
baltimore = homicides |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(
    unresolved = as.numeric(disposition == "Closed without arrest" | disposition == "Open/No arrest")) |> 
  select(city_state, unresolved, everything()) |> 
  group_by(unresolved) |> 
  summarize(number = n()) |> 
  mutate(total = sum(number)) |> 
  filter(unresolved == 1) |> 
  select(number, total)
```

``` r
baltimore_pt = 
  prop.test(baltimore$number, baltimore$total) |>
  broom::tidy() |> 
  janitor::clean_names()

baltimore_pt |> 
  select(estimate, conf_low, conf_high) |> knitr::kable()
```

|  estimate |  conf_low | conf_high |
|----------:|----------:|----------:|
| 0.6455607 | 0.6275625 | 0.6631599 |

The estimated proportion of homicides unsolved is 0.6456 with a
confidence interval of \[0.627, 0.663\].

\*\*do rest of prop tests for each city later

## Problem 2

``` r
names = list.files(path="./local/data", full.names=TRUE)

readin_csv = function(path) {
  df =
    read_csv(path) |>
    janitor::clean_names() |>
    mutate(
      id = path
    ) |> 
    mutate(arm = substr(id, 14, 16), id = substr(id, 18, 19))
} |> 
  select(everything())

output = map(names, readin_csv) |> bind_rows()
```

``` r
output = output |> 
  pivot_longer(week_1:week_8,
               names_to = "week", 
               values_to = "value") |> 
  mutate(week = substr(week, 6, 7))
```

``` r
output |> 
  mutate(week = as.numeric(week)) |> 
  ggplot(aes(x = week, y = value, color = id)) + geom_line() + facet_grid(~arm)
```

![](p8105_hw5_nl2835_files/figure-gfm/spaghetti%20plot-1.png)<!-- -->

The experimental arm had higher average values overall and an increase
over time, compared to the control arm that had lower average values and
a decrease over time.

## Problem 3

``` r
set.seed(1)

sim_ttest = function(mu) {
  data = tibble(
    x = rnorm(n=30, mean=mu, sd = 5)
  )
  
output = data |> 
  t.test() |> 
  broom::tidy() |> 
  select(estimate, p.value) |> 
  rename(p_value = p.value)
}

sim_results_df = 
  expand_grid(
    mu = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:50) |> 
  mutate(
    estimate_df = map(mu, sim_ttest) ) |> 
  unnest(estimate_df)
```

``` r
sim_results_df |> 
  group_by(mu) |> 
  summarize(
    power = sum(p_value < 0.05),
    power_prop = power/50
  ) |> 
  ggplot(aes(x = mu, y= power_prop)) + geom_line()
```

![](p8105_hw5_nl2835_files/figure-gfm/plot%20of%20proportion%20times%20null%20rejected%20and%20true%20value-1.png)<!-- -->

The association between effect size and power is positive.

``` r
mu_avg = sim_results_df |> 
  group_by(mu) |> 
  summarize(
  mu_avg = mean(estimate))

null_rej = sim_results_df |> 
  filter(p_value <0.05) |> 
  group_by(mu) |> 
  summarize(
    mu_avg = mean(estimate)
  )

ggplot(mu_avg, aes(x = mu, y = mu_avg)) +
  geom_line() +
  geom_line(data = null_rej, color = "blue")
```

![](p8105_hw5_nl2835_files/figure-gfm/avg%20est%20mu%20vs%20true%20mu-1.png)<!-- -->

There is almost an exactly linear relationship between the true value of
mu and the average estimate of mu in all samples.

In samples where the null was rejected, the sample average of mu across
tests is not approximately equal to the true value of mu for lower
effect sizes, because a lower effect size indicates weaker power and a
lower probability that H0 is false.
